#!/usr/bin/env bash
set -euo pipefail

# --- CONFIG ---
LOCAL_FILE="/var/log/apisix/myroute.log"     # file-logger path
HDFS_DIR="/user/harshit/apisix/logs"         # HDFS dir (must exist or will be created)
ROUTE_SLUG="myroute"                         # used in HDFS filename
FLUSH_WAIT=2                                 # seconds to let file-logger flush
LOOP_SLEEP=600                               # 10 minutes between cycles

# Your three APISIX calls
commands=(
  'curl -s -o /dev/null "http://127.0.0.1:9080/route1"'
  'curl -s -o /dev/null "http://127.0.0.1:9080/route2"'
  'curl -s -o /dev/null "http://127.0.0.1:9080/route3"'
)

while true; do
  # 1) generate logs
  for cmd in "${commands[@]}"; do
    echo "[$(date -Is)] $cmd"
    bash -lc "$cmd" || echo "[$(date -Is)] WARN: command failed"
  done

  # 2) small wait so APISIX file-logger flushes its batch
  sleep "$FLUSH_WAIT"

  # 3) snapshot -> truncate -> ship
  ts=$(date -u +%Y%m%dT%H%M%SZ)
  snap="/tmp/${ROUTE_SLUG}_${ts}.log"

  if [[ -f "$LOCAL_FILE" ]]; then
    cp -- "$LOCAL_FILE" "$snap" || true          # snapshot current content
    : > "$LOCAL_FILE"                            # TRUNCATE so next cycle starts fresh
  fi

  if [[ -s "$snap" ]]; then
    hdfs dfs -mkdir -p "$HDFS_DIR"
    hdfs dfs -put -f "$snap" "$HDFS_DIR/${ROUTE_SLUG}_${ts}.log"
    echo "[$(date -Is)] shipped to hdfs://${HDFS_DIR}/${ROUTE_SLUG}_${ts}.log"
  else
    echo "[$(date -Is)] nothing to ship"
  fi
  rm -f "$snap"

  sleep "$LOOP_SLEEP"
done
