from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "codellama/CodeLlama-7b-Instruct-hf"

# Load in FP16 to reduce memory pressure
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,  # Critical for memory reduction
    low_cpu_mem_usage=True,
    device_map="cpu"
)

# Save as FP16
model.save_pretrained("codellama-7b-instruct-fp16", torch_dtype=torch.float16)


Then:
from optimum.onnxruntime import ORTModelForCausalLM

# Export with memory optimizations
# 2. Export with external data format
ort_model = ORTModelForCausalLM.from_pretrained(
    model_id,
    export=True,
    provider="CPUExecutionProvider",
    # These are the correct parameters for large models:
    use_external_data_format=True,  # Must be inside session_options
    session_options={
        "use_external_data_format": True,  # Required for >2GB models
        "optimized_model_filepath": "codellama-7b-instruct.onnx",
        "log_severity_level": 3  # Reduce logging verbosity
    }
)

Alternative approach:
from optimum.onnxruntime import ORTModelForCausalLM, ORTConfig

# Create configuration first
ort_config = ORTConfig(
    use_external_data_format=True,
    max_export_size=1024,  # MB per file
    optimization_level=1,
    provider="CPUExecutionProvider"
)

# Then export
ort_model = ORTModelForCausalLM.from_pretrained(
    model_id,
    export=True,
    ort_config=ort_config
)

from onnxruntime.quantization import quantize_dynamic, QuantType

Quantization with External Data
quantize_dynamic(
    model_input="codellama-7b-instruct/model.onnx",
    model_output="codellama-7b-instruct-quant/model_quant.onnx",
    weight_type=QuantType.QInt8,
    use_external_data_format=True,  # Must match export setting
    extra_options={
        "WeightSymmetric": True,
        "ActivationSymmetric": False,
        "UseExternalDataFormat": True  # Additional confirmation
    }
)

Then:
from onnxruntime.quantization import quantize_dynamic, QuantType
import os

# Create output directory
os.makedirs("codellama-7b-instruct-quantized", exist_ok=True)

quant_config = {
    "weight_type": QuantType.QInt8,
    "per_channel": False,
    "use_external_data_format": True,
    "extra_options": {
        "WeightSymmetric": True,
        "ActivationSymmetric": False,
        "UseExternalDataFormat": True
    }
}

# Process each component separately
for component in ["model.onnx"]:  # Add other components if split
    input_path = f"codellama-7b-instruct/{component}"
    output_path = f"codellama-7b-instruct-quantized/{component}"
    
    if os.path.exists(input_path):
        quantize_dynamic(
            model_input=input_path,
            model_output=output_path,
            **quant_config
        )

============================
Testing of gptq model without gptq:

Challenges
Without auto-gptq, you have to:

Read the quantized weight format (often .safetensors or .pt)

Implement 4-bit matmul

Simulate quantization and dequantization during forward pass:

Code:
import os
from safetensors.torch import load_file
import torch
from transformers import AutoTokenizer
import glob

# === CONFIG ===
MODEL_DIR = "/path/to/llama-3-8b-gptq"  # Update this path
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MAX_NEW_TOKENS = 50
PROMPT = "The meaning of life is"

# === LOAD TOKENIZER ===
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)

# === LOAD SAFETENSORS SHARDS ===
tensor_files = sorted(glob.glob(os.path.join(MODEL_DIR, "*.safetensors")))

print(f"Found {len(tensor_files)} shard files.")
all_weights = {}

for f in tensor_files:
    print(f"Loading {f}...")
    weights = load_file(f, device=DEVICE)
    all_weights.update(weights)

print("All weights loaded.")

# === PRINT SAMPLE KEYS ===
print("\nSample keys:", list(all_weights.keys())[:5])

# === DUMMY TEXT GENERATOR USING FINAL LINEAR ===
# This is NOT full generation - just a simulation for now
# LLaMA architecture requires full model layers

# Use final lm_head weight to simulate logits
lm_head_weight = all_weights.get("lm_head.weight")  # (vocab_size, hidden_size)
if lm_head_weight is None:
    raise ValueError("lm_head.weight not found in weights!")

lm_head_weight = lm_head_weight.to(DEVICE)

# Fake hidden state for demo
def generate_tokens(prompt, max_tokens=MAX_NEW_TOKENS):
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(DEVICE)

    for _ in range(max_tokens):
        # Dummy hidden state simulation: last token's embedding
        # You should replace this with real model forward
        last_token_id = input_ids[:, -1]
        hidden_fake = torch.randn(1, lm_head_weight.shape[1], device=DEVICE)

        logits = hidden_fake @ lm_head_weight.T  # (1, vocab_size)
        next_token = torch.argmax(logits, dim=-1)

        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)

    return tokenizer.decode(input_ids[0], skip_special_tokens=True)

# === RUN GENERATION ===
output = generate_tokens(PROMPT)
print("\nGenerated text:\n", output)


+++++

from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# Path to your quantized model
model_name_or_path = "/path/to/your/quantized-llama3-8b-gptq"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

# Load quantized model
model = AutoGPTQForCausalLM.from_quantized(
    model_name_or_path,
    device="cuda:0",                # or "cuda" if you have 1 GPU
    torch_dtype="auto",             # Will correctly use float16
    use_triton=False,               # Set True if you quantized with Triton
    inject_fused_attention=True,    # Speeds up attention
)

# Your input prompt
prompt = "Explain the theory of relativity in simple words."

# Tokenize prompt
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Generate output
output_tokens = model.generate(
    **inputs,
    max_new_tokens=200,       # How many tokens to generate
    do_sample=True,           # Enable sampling for more creative output
    temperature=0.7,          # Sampling temperature
    top_p=0.9,                # Nucleus sampling
)

# Decode and print the generated text
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(generated_text)

