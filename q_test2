from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "codellama/CodeLlama-7b-Instruct-hf"

# Load in FP16 to reduce memory pressure
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,  # Critical for memory reduction
    low_cpu_mem_usage=True,
    device_map="cpu"
)

# Save as FP16
model.save_pretrained("codellama-7b-instruct-fp16", torch_dtype=torch.float16)


Then:
from optimum.onnxruntime import ORTModelForCausalLM

# Export with memory optimizations
# 2. Export with external data format
ort_model = ORTModelForCausalLM.from_pretrained(
    model_id,
    export=True,
    provider="CPUExecutionProvider",
    # These are the correct parameters for large models:
    use_external_data_format=True,  # Must be inside session_options
    session_options={
        "use_external_data_format": True,  # Required for >2GB models
        "optimized_model_filepath": "codellama-7b-instruct.onnx",
        "log_severity_level": 3  # Reduce logging verbosity
    }
)

Alternative approach:
from optimum.onnxruntime import ORTModelForCausalLM, ORTConfig

# Create configuration first
ort_config = ORTConfig(
    use_external_data_format=True,
    max_export_size=1024,  # MB per file
    optimization_level=1,
    provider="CPUExecutionProvider"
)

# Then export
ort_model = ORTModelForCausalLM.from_pretrained(
    model_id,
    export=True,
    ort_config=ort_config
)

from onnxruntime.quantization import quantize_dynamic, QuantType

Quantization with External Data
quantize_dynamic(
    model_input="codellama-7b-instruct/model.onnx",
    model_output="codellama-7b-instruct-quant/model_quant.onnx",
    weight_type=QuantType.QInt8,
    use_external_data_format=True,  # Must match export setting
    extra_options={
        "WeightSymmetric": True,
        "ActivationSymmetric": False,
        "UseExternalDataFormat": True  # Additional confirmation
    }
)

Then:
from onnxruntime.quantization import quantize_dynamic, QuantType
import os

# Create output directory
os.makedirs("codellama-7b-instruct-quantized", exist_ok=True)

quant_config = {
    "weight_type": QuantType.QInt8,
    "per_channel": False,
    "use_external_data_format": True,
    "extra_options": {
        "WeightSymmetric": True,
        "ActivationSymmetric": False,
        "UseExternalDataFormat": True
    }
}

# Process each component separately
for component in ["model.onnx"]:  # Add other components if split
    input_path = f"codellama-7b-instruct/{component}"
    output_path = f"codellama-7b-instruct-quantized/{component}"
    
    if os.path.exists(input_path):
        quantize_dynamic(
            model_input=input_path,
            model_output=output_path,
            **quant_config
        )
