from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Your local model path
model_path = "/path/to/Meta-Llama-3.1-8B-Instruct"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

# Load the model on GPU in FP32
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",             # Automatically put model on GPU
    torch_dtype=torch.float32       # Explicitly load in FP32
)

model.eval()

# Prompt for inference
prompt = "Describe how photosynthesis works."

# Tokenize prompt
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Generate output
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

# Decode and print
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)


======
Half Precision:
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Your local model path
model_path = "/path/to/Meta-Llama-3.1-8B-Instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

# Load model and automatically cast weights to FP16
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16,  # <-- important change
)

model.eval()

# Prompt
prompt = "Explain the theory of relativity in simple words."

# Tokenize
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Inference
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

# Decode
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)


_++++++++++++++=============
Compare:
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Select dtype: torch.float32 or torch.float16
desired_dtype = torch.float16   # or torch.float32

# Local model path
model_path = "/path/to/Meta-Llama-3.1-8B-Instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=desired_dtype,
)

model.eval()

# Input prompt
prompt = "Explain black holes simply."
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Warm up once (optional, helps with first-time slow startup)
with torch.no_grad():
    _ = model.generate(**inputs, max_new_tokens=10)

# Start timing
start_time = time.time()

# Real inference
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

end_time = time.time()

# Decode output
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

# Print time taken
print(f"\nTime taken: {end_time - start_time:.2f} seconds")
