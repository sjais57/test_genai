from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Your local model path
model_path = "/path/to/Meta-Llama-3.1-8B-Instruct"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

# Load the model on GPU in FP32
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",             # Automatically put model on GPU
    torch_dtype=torch.float32       # Explicitly load in FP32
)

model.eval()

# Prompt for inference
prompt = "Describe how photosynthesis works."

# Tokenize prompt
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Generate output
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

# Decode and print
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)


======
Half Precision:
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Your local model path
model_path = "/path/to/Meta-Llama-3.1-8B-Instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

# Load model and automatically cast weights to FP16
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16,  # <-- important change
)

model.eval()

# Prompt
prompt = "Explain the theory of relativity in simple words."

# Tokenize
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Inference
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

# Decode
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)


_++++++++++++++=============
Compare:
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Select dtype: torch.float32 or torch.float16
desired_dtype = torch.float16   # or torch.float32

# Local model path
model_path = "/path/to/Meta-Llama-3.1-8B-Instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=desired_dtype,
)

model.eval()

# Input prompt
prompt = "Explain black holes simply."
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Warm up once (optional, helps with first-time slow startup)
with torch.no_grad():
    _ = model.generate(**inputs, max_new_tokens=10)

# Start timing
start_time = time.time()

# Real inference
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

end_time = time.time()

# Decode output
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

# Print time taken
print(f"\nTime taken: {end_time - start_time:.2f} seconds")



==========================================================================
Quantized model: Code: Load LLaMA-3-8B-Instruct with 4-bit Quantization (AutoGPTQ)
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_name = "TheBloke/Meta-Llama-3-8B-Instruct-GPTQ"  # 4-bit quantized model

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

# Load quantized model (uses AutoGPTQ under the hood)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",       # Automatically maps to GPU
    torch_dtype=torch.float16,  # Can also use float32 if memory allows
    trust_remote_code=True
)

# Set up text generation pipeline
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Example prompt
prompt = "Explain the concept of transfer learning in simple terms."

# Generate response
output = pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)
print(output[0]["generated_text"])


============================================================
Full precision model: Alternative: Use bitsandbytes for 8-bit or 4-bit (FP4/INT8) inference

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # or load_in_8bit=True
    bnb_4bit_quant_type="nf4",  # Normal Float 4, good trade-off
    bnb_4bit_use_double_quant=True,
    llm_int8_threshold=6.0,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16
)

inputs = tokenizer("Explain quantum computing in simple terms:", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

